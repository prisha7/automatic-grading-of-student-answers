{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prisha7/automatic-grading-of-student-answers/blob/main/Prisha_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install -q sentence-transformers torch scikit-learn pandas numpy nltk matplotlib seaborn"
      ],
      "metadata": {
        "id": "GZFalCvibpW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# NLTK imports\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Sklearn imports\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, cohen_kappa_score\n",
        "\n",
        "# Sentence Transformers (SBERT)\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "from torch.nn.functional import cosine_similarity\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
      ],
      "metadata": {
        "id": "sSEY_cBdbv7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "#print(\"Please upload your ASAP dataset file (ASAP2_train_sourcetexts.csv)...\")\n",
        "#uploaded = files.upload()\n",
        "#filename = list(uploaded.keys())[0]\n",
        "\n",
        "data = pd.read_csv('ASAP2_train_sourcetexts.csv')\n",
        "print(f\"\\nDataset loaded successfully!\")\n",
        "print(f\"Dataset shape: {data.shape}\")\n",
        "print(f\"\\nColumn names:\")\n",
        "print(data.columns.tolist())\n",
        "\n",
        "data = data[['essay_id', 'score', 'full_text', 'prompt_name']]\n",
        "\n",
        "print(f\"\\nSelected columns: {data.columns.tolist()}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "print(data.head())\n",
        "\n",
        "print(f\"\\nScore distribution:\")\n",
        "print(data['score'].value_counts().sort_index())\n",
        "\n",
        "# Check for missing values\n",
        "print(f\"\\nMissing values:\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Remove rows with missing values\n",
        "data = data.dropna()\n",
        "print(f\"\\nDataset after removing missing values: {data.shape}\")"
      ],
      "metadata": {
        "id": "QNzsIs58cDNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4: Text Preprocessing Functions\n",
        "def clean_text(text):\n",
        "      \"\"\"Clean and normalize text\"\"\"\n",
        "      text = text.lower()\n",
        "      text = re.sub(r'[^a-z\\s]', '', text)            # Remove non-alphabetic characters\n",
        "      text = re.sub(r'\\s+', ' ', text).strip()        # Normalize whitespace\n",
        "      return text\n",
        "\n",
        "print(\"Applying text cleaning...\")\n",
        "data['clean_essay'] = data['full_text'].apply(clean_text)\n",
        "data['clean_prompt'] = data['prompt_name'].apply(clean_text)\n",
        "\n",
        "print(\" Text cleaning complete!\")\n",
        "print(f\"\\nOriginal essay example:\")\n",
        "print(data['full_text'].iloc[0][:200])\n",
        "print(f\"\\nCleaned essay example:\")\n",
        "print(data['clean_essay'].iloc[0][:200])"
      ],
      "metadata": {
        "id": "L6G9gu6nikXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 5 : Tokenization\n",
        "print(\"Tokenizing text...\")\n",
        "# Download the missing resource\n",
        "nltk.download('punkt_tab')\n",
        "data['tokens'] = data['clean_essay'].apply(word_tokenize)\n",
        "data['tokens_prompt_name'] = data['clean_prompt'].apply(word_tokenize)\n",
        "\n",
        "print(\"Tokenization complete!\")\n",
        "print(f\"\\nExample tokens (first 20):\")\n",
        "print(data['tokens'].iloc[0][:20])"
      ],
      "metadata": {
        "id": "XCu3DuhJi4cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 6:Remove stop words\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "print(\"Removing stop words...\")\n",
        "data['filtered_tokens'] = data['tokens'].apply(\n",
        "    lambda tokens: [t for t in tokens if t not in stop_words]\n",
        "    )\n",
        "\n",
        "data['filtered_tokens_prompt_name'] = data['tokens_prompt_name'].apply(\n",
        "      lambda tokens: [t for t in tokens if t not in stop_words]\n",
        "      )\n",
        "\n",
        "print(\"Stop words removed!\")\n",
        "print(f\"\\nTokens before: {len(data['tokens'].iloc[0])}\")\n",
        "print(f\"Tokens after: {len(data['filtered_tokens'].iloc[0])}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i0YDGN7CjX_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 7 : Stemming\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "print(\"Applying stemming...\")\n",
        "data['stemmed_tokens'] = data['filtered_tokens'].apply(\n",
        "    lambda tokens: [stemmer.stem(t) for t in tokens]\n",
        "    )\n",
        "\n",
        "data['stemmed_text'] = data['stemmed_tokens'].apply(\n",
        "      lambda tokens: ' '.join(tokens)\n",
        "      )\n",
        "data['stemmed_tokens_prompt_name'] = data['filtered_tokens_prompt_name'].apply(\n",
        "      lambda tokens: [stemmer.stem(t) for t in tokens]\n",
        "      )\n",
        "data['stemmed_prompt'] = data['stemmed_tokens_prompt_name'].apply(\n",
        "      lambda tokens: ' '.join(tokens)\n",
        "      )\n",
        "print(\"Stemming complete!\")\n",
        "print(f\"\\nOriginal tokens: {data['filtered_tokens'].iloc[0][:10]}\")\n",
        "print(f\"Stemmed tokens: {data['stemmed_tokens'].iloc[0][:10]}\")\n"
      ],
      "metadata": {
        "id": "Y6NadknRkNEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 8: TF-IDF Vectorization (Keyword Similarity)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"Computing TF-IDF vectors and keyword similarity...\")\n",
        "\n",
        "# Create TF-IDF vectorizer\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "essay_vecs = tfidf.fit_transform(data['stemmed_text'])\n",
        "prompt_vecs = tfidf.transform(data['stemmed_prompt'])\n",
        "\n",
        "# Compute cosine similarity for each pair (element-wise)\n",
        "from sklearn.metrics.pairwise import cosine_similarity as sklearn_cosine\n",
        "\n",
        "keyword_sim = [\n",
        "    sklearn_cosine(prompt_vecs[i], essay_vecs[i])[0, 0]\n",
        "        for i in range(len(data))\n",
        "        ]\n",
        "\n",
        "data['keyword_similarity'] = keyword_sim\n",
        "\n",
        "print(\"✓ Keyword similarity computed!\")\n",
        "print(f\"\\nKeyword similarity statistics:\")\n",
        "print(data['keyword_similarity'].describe())\n",
        "\n",
        "        # Save TF-IDF essay features\n",
        "essay_tfidf_features = essay_vecs.toarray()\n",
        "print(f\"\\nTF-IDF feature matrix shape: {essay_tfidf_features.shape}\")"
      ],
      "metadata": {
        "id": "rdLkV8Pjm20m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 9: SBERT Vectorization (Semantic Similarity)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"Loading SBERT model and computing semantic similarity...\")\n",
        "\n",
        "# Load SBERT model (lightweight, works well on CPU)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "sbert = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
        "print(f\"✓ SBERT model loaded on {device}\")\n",
        "\n",
        "# Encode text\n",
        "print(\"Encoding prompts...\")\n",
        "prompt_vecs = sbert.encode(data['clean_prompt'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
        "\n",
        "print(\"Encoding essays...\")\n",
        "essay_vecs = sbert.encode(data['clean_essay'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
        "\n",
        "# Compute cosine similarity for each row\n",
        "print(\"Computing semantic similarity...\")\n",
        "semantic_sim = cosine_similarity(prompt_vecs, essay_vecs).cpu().numpy()\n",
        "data['semantic_similarity'] = semantic_sim\n",
        "\n",
        "print(\"Semantic similarity computed!\")\n",
        "print(f\"\\nSemantic similarity statistics:\")\n",
        "print(data['semantic_similarity'].describe())\n",
        "\n",
        "# Save SBERT embeddings for later use\n",
        "essay_sbert_features = essay_vecs.cpu().numpy()\n",
        "print(f\"\\nSBERT embedding shape: {essay_sbert_features.shape}\")"
      ],
      "metadata": {
        "id": "6F5rwqg3nHHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 10: Extract Additional Features\n",
        "# ============================================================================\n",
        "\n",
        "print(\"Extracting additional features...\")\n",
        "\n",
        "# Basic text features\n",
        "data['essay_length'] = data['full_text'].apply(len)\n",
        "data['word_count'] = data['tokens'].apply(len)\n",
        "data['unique_word_count'] = data['tokens'].apply(lambda x: len(set(x)))\n",
        "data['avg_word_length'] = data['tokens'].apply(\n",
        "    lambda x: np.mean([len(word) for word in x]) if len(x) > 0 else 0\n",
        "    )\n",
        "data['sentence_count'] = data['full_text'].apply(lambda x: len(re.split(r'[.!?]+', x)))\n",
        "\n",
        "# Vocabulary richness\n",
        "data['lexical_diversity'] = data['unique_word_count'] / data['word_count']\n",
        "\n",
        "# Prompt-related features\n",
        "data['prompt_word_overlap'] = data.apply(\n",
        "    lambda row: len(set(row['tokens']) & set(row['tokens_prompt_name'])) / len(set(row['tokens_prompt_name']))\n",
        "        if len(row['tokens_prompt_name']) > 0 else 0,\n",
        "            axis=1\n",
        "            )\n",
        "\n",
        "print(\"Additional features extracted\")\n",
        "\n",
        "# Display all feature columns\n",
        "feature_cols = ['keyword_similarity', 'semantic_similarity', 'essay_length',\n",
        "                'word_count', 'unique_word_count', 'avg_word_length',\n",
        "                'sentence_count', 'lexical_diversity', 'prompt_word_overlap'\n",
        "                ]\n",
        "\n",
        "print(f\"\\nFeature summary:\")\n",
        "print(data[feature_cols].describe())"
      ],
      "metadata": {
        "id": "irzDwFdLxUTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 11: Prepare Feature Matrix\n",
        "# ============================================================================\n",
        "\n",
        "print(\"Preparing feature matrix...\")\n",
        "\n",
        "# Combine all features\n",
        "basic_features = data[['keyword_similarity', 'semantic_similarity', 'essay_length',\n",
        "                        'word_count', 'unique_word_count', 'avg_word_length',\n",
        "                        'sentence_count', 'lexical_diversity', 'prompt_word_overlap']].values\n",
        "\n",
        "# Option 1: Use only basic features + similarity scores\n",
        "X_basic = basic_features\n",
        "\n",
        "X = X_basic\n",
        "y = data['score'].values\n",
        "\n",
        "print(f\"Feature matrix prepared!\")\n",
        "print(f\"Feature matrix shape: {X.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")"
      ],
      "metadata": {
        "id": "3ISXOgsjyQ3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 12: Split Data and Scale Features\n",
        "# ============================================================================\n",
        "\n",
        "print(\"Splitting data into train and test sets...\")\n",
        "\n",
        "# Split data (80-20 train-test split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Testing samples: {len(X_test)}\")\n",
        "\n",
        "# Scale features\n",
        "print(\"\\nScaling features...\")\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Data preparation complete!\")\n",
        "\n",
        "# Check score distribution\n",
        "print(f\"\\nTrain score distribution:\")\n",
        "print(pd.Series(y_train).value_counts().sort_index())\n",
        "print(f\"\\nTest score distribution:\")\n",
        "print(pd.Series(y_test).value_counts().sort_index())"
      ],
      "metadata": {
        "id": "wACZGI9CylUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 13: Train Multiple Regression Models\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"TRAINING REGRESSION MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "\n",
        "models = {\n",
        "      'Linear Regression': LinearRegression(),\n",
        "      'Ridge Regression': Ridge(alpha=10.0),\n",
        "      'Lasso Regression': Lasso(alpha=1.0),\n",
        "      'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42),\n",
        "      'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=5, random_state=42),\n",
        "      'Support Vector Regression': SVR(kernel='rbf', C=10.0)\n",
        "       }\n",
        "\n",
        "# Train models\n",
        "trained_models = {}\n",
        "predictions_dict = {}\n",
        "\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    trained_models[name] = model\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    predictions_dict[name] = y_pred\n",
        "\n",
        "    print(f\"{name} trained successfully!\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"All models trained!\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "0wJ5ucCB0EKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 14: Evaluate Models\n",
        "# ============================================================================\n",
        "\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    \"\"\"Evaluate model with multiple metrics\"\"\"\n",
        "    # Get score range\n",
        "    min_score = y_true.min()\n",
        "    max_score = y_true.max()\n",
        "\n",
        "    # Round predictions to nearest valid score\n",
        "    y_pred_rounded = np.clip(np.round(y_pred), min_score, max_score)\n",
        "\n",
        "\n",
        "    # Calculate metrics\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    qwk = cohen_kappa_score(y_true, y_pred_rounded, weights='quadratic')\n",
        "\n",
        "    # Accuracy within 1 point\n",
        "    within_one = np.mean(np.abs(y_true - y_pred_rounded) <= 1)\n",
        "\n",
        "    return {\n",
        "        'Model': model_name,\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'R²': r2,\n",
        "        'QWK': qwk,\n",
        "        'Within 1 Point': within_one\n",
        "        }\n",
        "# Evaluate all models\n",
        "results = []\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"MODEL EVALUATION RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for name, y_pred in predictions_dict.items():\n",
        "    metrics = evaluate_model(y_test, y_pred, name)\n",
        "    results.append(metrics)\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  MSE:  {metrics['MSE']:.4f}\")\n",
        "    print(f\"  RMSE: {metrics['RMSE']:.4f}\")\n",
        "    print(f\"  MAE:  {metrics['MAE']:.4f}\")\n",
        "    print(f\"  R²:   {metrics['R²']:.4f}\")\n",
        "    print(f\"  Quadratic Weighted Kappa: {metrics['QWK']:.4f}\")\n",
        "    print(f\"  Accuracy within 1 point: {metrics['Within 1 Point']:.2%}\")\n",
        "\n",
        "# Create results dataframe\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.sort_values('QWK', ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RESULTS SUMMARY (Sorted by QWK)\")\n",
        "print(\"=\"*80)\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# Find best model\n",
        "best_model_name = results_df.iloc[0]['Model']\n",
        "best_qwk = results_df.iloc[0]['QWK']\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\" BEST MODEL: {best_model_name}\")\n",
        "print(f\"   Quadratic Weighted Kappa: {best_qwk:.4f}\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "9mni2t4K1kbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 15: Visualize Model Comparison\n",
        "# ============================================================================\n",
        "\n",
        "# Create comprehensive visualization\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Prepare data for plotting\n",
        "model_names = results_df['Model'].tolist()\n",
        "metrics_to_plot = ['MSE', 'RMSE', 'MAE', 'R²', 'QWK', 'Within 1 Point']\n",
        "colors = ['steelblue', 'coral', 'mediumseagreen', 'purple', 'orange', 'crimson']\n",
        "\n",
        "\n",
        "for idx, (metric, color) in enumerate(zip(metrics_to_plot, colors)):\n",
        "    row = idx // 3\n",
        "    col = idx % 3\n",
        "    ax = axes[row, col]\n",
        "\n",
        "    values = results_df[metric].tolist()\n",
        "\n",
        "    bars = ax.bar(range(len(model_names)), values, color=color, alpha=0.7)\n",
        "    ax.set_title(f'{metric} {\"(Lower is Better)\" if metric in [\"MSE\", \"RMSE\", \"MAE\"] else \"(Higher is Better)\"}')\n",
        "    ax.set_ylabel(metric)\n",
        "    ax.set_xticks(range(len(model_names)))\n",
        "    ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    if metric in ['MSE', 'RMSE', 'MAE']:\n",
        "        best_idx = values.index(min(values))\n",
        "    else:\n",
        "        best_idx = values.index(max(values))\n",
        "        bars[best_idx].set_color('gold')\n",
        "        bars[best_idx].set_edgecolor('black')\n",
        "        bars[best_idx].set_linewidth(2)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F3bBoSD0h1JE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 16: Prediction vs Actual Visualization\n",
        "# ============================================================================\n",
        "\n",
        "# Get predictions from best model\n",
        "best_model = trained_models[best_model_name]\n",
        "y_pred_best = predictions_dict[best_model_name]\n",
        "\n",
        "min_score = y_test.min()\n",
        "max_score = y_test.max()\n",
        "y_pred_rounded = np.clip(np.round(y_pred_best), min_score, max_score)\n",
        "\n",
        "# Create visualization\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "fig.suptitle(f'Best Model: {best_model_name}', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Plot 1: Scatter plot of predictions vs actual\n",
        "axes[0].scatter(y_test, y_pred_rounded, alpha=0.5, s=30)\n",
        "axes[0].plot([min_score, max_score], [min_score, max_score], 'r--', linewidth=2, label='Perfect prediction')\n",
        "axes[0].set_xlabel('Actual Score', fontsize=12)\n",
        "axes[0].set_ylabel('Predicted Score', fontsize=12)\n",
        "axes[0].set_title('Predictions vs Actual Scores')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Plot 2: Residual plot\n",
        "residuals = y_test - y_pred_rounded\n",
        "axes[1].scatter(y_test, residuals, alpha=0.5, s=30)\n",
        "axes[1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
        "axes[1].set_xlabel('Actual Score', fontsize=12)\n",
        "axes[1].set_ylabel('Residual (Actual - Predicted)', fontsize=12)\n",
        "axes[1].set_title('Residual Plot')\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "# Plot 3: Distribution comparison\n",
        "axes[2].hist(y_test, bins=20, alpha=0.5, label='Actual', color='blue')\n",
        "axes[2].hist(y_pred_rounded, bins=20, alpha=0.5, label='Predicted', color='orange')\n",
        "axes[2].set_xlabel('Score', fontsize=12)\n",
        "axes[2].set_ylabel('Frequency', fontsize=12)\n",
        "axes[2].set_title('Score Distribution Comparison')\n",
        "axes[2].legend()\n",
        "axes[2].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "guV1OuQvix-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 17: Confusion Matrix for Best Model\n",
        "# ============================================================================\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Create confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_rounded)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=sorted(np.unique(y_test)),\n",
        "            yticklabels=sorted(np.unique(y_test)))\n",
        "plt.xlabel('Predicted Score', fontsize=12)\n",
        "plt.ylabel('Actual Score', fontsize=12)\n",
        "plt.title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate accuracy per score\n",
        "print(\"=\"*80)\n",
        "print(\"ACCURACY PER SCORE LEVEL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for score in sorted(np.unique(y_test)):\n",
        "    mask = y_test == score\n",
        "    accuracy = np.mean(y_pred_rounded[mask] == score)\n",
        "    print(f\"Score {score}: {accuracy:.2%} ({mask.sum()} samples)\")"
      ],
      "metadata": {
        "id": "5NHQtMpui4Pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 18: Feature Importance Analysis\n",
        "# ============================================================================\n",
        "\n",
        "# Feature importance for tree-based models\n",
        "if best_model_name in ['Random Forest', 'Gradient Boosting']:\n",
        "    feature_importance = best_model.feature_importances_\n",
        "\n",
        "    feature_names = ['keyword_similarity', 'semantic_similarity', 'essay_length',\n",
        "    'word_count', 'unique_word_count', 'avg_word_length',\n",
        "    'sentence_count', 'lexical_diversity', 'prompt_word_overlap']\n",
        "\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': feature_importance }).sort_values('Importance', ascending=False)\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(f\"FEATURE IMPORTANCE ({best_model_name})\")\n",
        "    print(\"=\"*80)\n",
        "    print(importance_df.to_string(index=False))\n",
        "\n",
        "    # Plot feature importance\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(range(len(importance_df)), importance_df['Importance'], color='skyblue')\n",
        "    plt.yticks(range(len(importance_df)), importance_df['Feature'])\n",
        "    plt.xlabel('Importance Score', fontsize=12)\n",
        "    plt.title(f'Feature Importance - {best_model_name}', fontsize=14, fontweight='bold')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "bqXOoH3mjN9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 19: Sample Predictions Analysis\n",
        "# ============================================================================\n",
        "\n",
        "# Create detailed results dataframe\n",
        "indices = list(range(len(y_test)))\n",
        "test_results = pd.DataFrame({\n",
        "    'actual_score': y_test,\n",
        "    'predicted_score': y_pred_rounded,\n",
        "    'raw_prediction': y_pred_best,\n",
        "    'difference': np.abs(y_test - y_pred_rounded)\n",
        "    })\n",
        "\n",
        "# Get test indices\n",
        "test_indices = X_test_scaled.shape[0]\n",
        "original_test_indices = data.index[-test_indices:]\n",
        "\n",
        "# Show 10 random samples\n",
        "num_samples = min(10, len(test_results))\n",
        "sample_indices = np.random.choice(len(test_results), size=num_samples, replace=False)\n",
        "\n",
        "for i, idx in enumerate(sample_indices, 1):\n",
        "    orig_idx = original_test_indices[idx]\n",
        "    row = test_results.iloc[idx]\n",
        "\n",
        "    print(f\"\\n{'─'*80}\")\n",
        "    print(f\"Sample {i}:\")\n",
        "    print(f\"{'─'*80}\")\n",
        "\n",
        "    essay_preview = data.loc[orig_idx, 'full_text'][:300]\n",
        "    if len(data.loc[orig_idx, 'full_text']) > 300:\n",
        "        essay_preview += \"...\"\n",
        "\n",
        "    print(f\"Essay Preview: {essay_preview}\")\n",
        "    print(f\"\\nActual Score:    {row['actual_score']}\")\n",
        "    print(f\"Predicted Score: {row['predicted_score']}\")\n",
        "    print(f\"Raw Prediction:  {row['raw_prediction']:.2f}\")\n",
        "    print(f\"Difference:      {row['difference']}\")\n",
        "\n",
        "    if row['difference'] == 0:\n",
        "        print(\"Perfect match!\")\n",
        "    elif row['difference'] <= 1:\n",
        "        print(\"Close prediction\")\n",
        "    else:\n",
        "        print(\"Significant difference\")\n",
        "\n",
        "\n",
        "# Overall statistics\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"OVERALL PREDICTION STATISTICS\")\n",
        "print(f\"{'='*80}\")\n",
        "exact_matches = (test_results['difference'] == 0).sum()\n",
        "within_one = (test_results['difference'] <= 1).sum()\n",
        "within_two = (test_results['difference'] <= 2).sum()\n",
        "\n",
        "print(f\"Exact matches:     {exact_matches}/{len(test_results)} ({exact_matches/len(test_results)*100:.1f}%)\")\n",
        "print(f\"Within 1 point:    {within_one}/{len(test_results)} ({within_one/len(test_results)*100:.1f}%)\")\n",
        "print(f\"Within 2 points:   {within_two}/{len(test_results)} ({within_two/len(test_results)*100:.1f}%)\")"
      ],
      "metadata": {
        "id": "_AVPjMlZkKPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 20: Function to Grade New Essays\n",
        "# ============================================================================\n",
        "\n",
        "def grade_new_essay(essay_text, prompt_text, model_package):\n",
        "    \"\"\"\n",
        "    Grade a new essay using the trained model\n",
        "\n",
        "    Args:\n",
        "        essay_text: The student's essay\n",
        "        prompt_text: The prompt/question\n",
        "        model_package: Dictionary containing model and preprocessing tools\n",
        "\n",
        "    Returns:\n",
        "        Predicted score and detailed analysis\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract tools from package\n",
        "    model = model_package['model']\n",
        "    scaler = model_package['scaler']\n",
        "    sbert_model = model_package['sbert_model']\n",
        "    tfidf_vec = model_package['tfidf_vectorizer']\n",
        "\n",
        "    # Clean text\n",
        "    clean_essay = clean_text(essay_text)\n",
        "    clean_prompt = clean_text(prompt_text)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens_essay = word_tokenize(clean_essay)\n",
        "    tokens_prompt = word_tokenize(clean_prompt)\n",
        "\n",
        "    # Remove stopwords and stem\n",
        "    filtered_essay = [t for t in tokens_essay if t not in stop_words]\n",
        "    filtered_prompt = [t for t in tokens_prompt if t not in stop_words]\n",
        "\n",
        "    stemmed_essay = [stemmer.stem(t) for t in filtered_essay]\n",
        "    stemmed_prompt = [stemmer.stem(t) for t in filtered_prompt]\n",
        "\n",
        "    stemmed_essay_text = ' '.join(stemmed_essay)\n",
        "    stemmed_prompt_text = ' '.join(stemmed_prompt)\n",
        "\n",
        "    # Compute keyword similarity (TF-IDF) - using correct method\n",
        "    essay_vec = tfidf_vec.transform([stemmed_essay_text])\n",
        "    prompt_vec = tfidf_vec.transform([stemmed_prompt_text])\n",
        "    keyword_sim = sklearn_cosine(prompt_vec, essay_vec)[0, 0]\n",
        "\n",
        "    # Compute semantic similarity (SBERT)\n",
        "    prompt_vec_sbert = sbert_model.encode([clean_prompt], convert_to_tensor=True)\n",
        "    essay_vec_sbert = sbert_model.encode([clean_essay], convert_to_tensor=True)\n",
        "    semantic_sim = cosine_similarity(prompt_vec_sbert, essay_vec_sbert).cpu().numpy()[0]\n",
        "\n",
        "    # Extract basic features\n",
        "    essay_length = len(essay_text)\n",
        "    word_count = len(tokens_essay)\n",
        "    unique_word_count = len(set(tokens_essay))\n",
        "    avg_word_length = np.mean([len(word) for word in tokens_essay]) if tokens_essay else 0\n",
        "    sentence_count = len(re.split(r'[.!?]+', essay_text))\n",
        "    lexical_diversity = unique_word_count / word_count if word_count > 0 else 0\n",
        "    prompt_word_overlap = len(set(tokens_essay) & set(tokens_prompt)) / len(set(tokens_prompt)) if tokens_prompt else 0\n",
        "\n",
        "\n",
        "    # Combine all features\n",
        "    features = np.array([[\n",
        "    keyword_sim,\n",
        "    semantic_sim,\n",
        "    essay_length,\n",
        "    word_count,\n",
        "    unique_word_count,\n",
        "    avg_word_length,\n",
        "    sentence_count,\n",
        "    lexical_diversity,\n",
        "                                                                                    prompt_word_overlap\n",
        "                                                                                    ]])\n",
        "\n",
        "    # Scale features\n",
        "    features_scaled = scaler.transform(features)\n",
        "\n",
        "    #Predict score\n",
        "    raw_prediction = model.predict(features_scaled)[0]\n",
        "    predicted_score = np.round(raw_prediction)\n",
        "\n",
        "    # Create analysis report\n",
        "    analysis = {\n",
        "    'predicted_score': int(predicted_score),\n",
        "    'raw_prediction': float(raw_prediction),\n",
        "    'keyword_similarity': float(keyword_sim),\n",
        "    'semantic_similarity': float(semantic_sim),\n",
        "    'essay_length': int(essay_length),\n",
        "    'word_count': int(word_count),\n",
        "    'unique_word_count': int(unique_word_count),\n",
        "    'lexical_diversity': float(lexical_diversity),\n",
        "                                                                                    'sentence_count': int(sentence_count)\n",
        "                                                                                    }\n",
        "    return predicted_score, analysis\n",
        "\n",
        "# Test the function with a sample essay\n",
        "print(\"=\"*80)\n",
        "print(\"TESTING GRADING FUNCTION WITH NEW ESSAY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Sample test essay\n",
        "test_essay = \"\"\"\n",
        "Photosynthesis is a vital process for life on Earth. Plants use sunlight, water,\n",
        "and carbon dioxide to produce glucose and oxygen. The chlorophyll in plant leaves\n",
        "absorbs light energy, which drives the chemical reactions. This process not only\n",
        "provides food for plants but also produces oxygen that animals need to breathe.\n",
        "\"\"\"\n",
        "\n",
        "test_prompt = \"Explain the process of photosynthesis and its importance.\"\n",
        "\n",
        "\n",
        "try:\n",
        "    predicted_score, analysis = grade_new_essay(test_essay, test_prompt, model_package)\n",
        "\n",
        "    print(f\"\\nTest Essay:\")\n",
        "    print(test_essay)\n",
        "    print(f\"\\nPrompt:\")\n",
        "    print(test_prompt)\n",
        "    print(f\"\\n{'─'*80}\")\n",
        "    print(\"GRADING RESULTS:\")\n",
        "    print(f\"{'─'*80}\")\n",
        "    print(f\"Predicted Score: {predicted_score}\")\n",
        "    print(f\"Raw Prediction: {analysis['raw_prediction']:.2f}\")\n",
        "    print(f\"\\nFeature Analysis:\")\n",
        "    print(f\"  Keyword Similarity: {analysis['keyword_similarity']:.3f}\")\n",
        "    print(f\"  Semantic Similarity: {analysis['semantic_similarity']:.3f}\")\n",
        "    print(f\"  Word Count: {analysis['word_count']}\")\n",
        "    print(f\"  Unique Words: {analysis['unique_word_count']}\")\n",
        "    print(f\"  Lexical Diversity: {analysis['lexical_diversity']:.3f}\")\n",
        "    print(f\"  Sentence Count: {analysis['sentence_count']}\")\n",
        "    print(f\"  Essay Length: {analysis['essay_length']} characters\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during testing: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ALL DONE!!!!\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nYou can now use the grade_new_essay() function to grade any new essay!\")\n",
        "print(\"The trained model has been saved and can be loaded for future use.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7fXw1Naime1v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}